{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Env"
      ],
      "metadata": {
        "id": "Bg7PSDId2ihl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyYAML==5.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyvGfJYO5ifv",
        "outputId": "fbc51501-2177-45fe-afca-f8e3077a0564"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyYAML==5.3.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 269 kB 4.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44636 sha256=8add02218aa1af1dca73299d126537e5a1fcea62d951031642eb922617b81e79\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from collections import OrderedDict\n"
      ],
      "metadata": {
        "id": "8ye13DTx51Sj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleDriveDownloader.download_file_from_google_drive(file_id='1EULkcH_hhSU28qVc1jSJpCh2hGOrzpjK', dest_path='./model/body_pose_model.pth',unzip=False) \n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id='1ZNJDzQUjo2lDPwGoVkRLg77eA57dKUqx', dest_path='./data/hit_kick_data.zip',unzip=True)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id='1MZvrLgzt72m7gatg72-ST8cl0MrG-XwY', dest_path='./test.mp4',unzip=False)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id='1b1Y9NEB40p59V9o35hRqp3Gu26czMSUD', dest_path='./model/torch_classifier.h5',unzip=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wuBILaw2jPg",
        "outputId": "55082875-befe-4ac8-c062-55bf87401431"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1b1Y9NEB40p59V9o35hRqp3Gu26czMSUD into ./model/torch_classifier.h5... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "kfsvjlZ92gOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MainTask:\n",
        "    def __init__(self):\n",
        "        self.colors = [(255,0,0),(0,255,0),(0,0,255),(255,255,0)]\n",
        "        self.download_model_data()\n",
        "        self.person_detector = PersonDetection()\n",
        "        self.open_pose_detector = Body('model/body_pose_model.pth')\n",
        "        self.action_detector = ActionDetection('model/torch_classifier.h5')\n",
        "        self.frame_list = []\n",
        "        self.body_points = []\n",
        "        self.actions = None\n",
        "\n",
        "    @staticmethod\n",
        "    def download_model_data():\n",
        "        #GoogleDriveDownloader.download_file_from_google_drive(file_id='1EULkcH_hhSU28qVc1jSJpCh2hGOrzpjK',\n",
        "        #                                                      dest_path='./model/body_pose_model.pth', unzip=False)\n",
        "        #GoogleDriveDownloader.download_file_from_google_drive(file_id='1ZNJDzQUjo2lDPwGoVkRLg77eA57dKUqx',\n",
        "        #                                                      dest_path='./data/hit_kick_data.zip', unzip=True)\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_polygon(image, points, color, thickness, text):\n",
        "        points_len = len(points)\n",
        "        for i in range(0, points_len):\n",
        "            p0 = tuple(points[i])\n",
        "            p1 = tuple(points[(i + 1) % points_len])\n",
        "            image = cv2.line(image, p0, p1, color, thickness=thickness)\n",
        "\n",
        "        image = cv2.putText(image, text, points[3], cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                            1, color, 1, 1)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def draw_rect(self,image, rect, color, thickness, text):\n",
        "        l, t, r, b, _, _ = [int(x) for x in rect]\n",
        "        image = self.draw_polygon(image, [(l, t), (r, t), (r, b), (l, b)], color, thickness, text)\n",
        "        return image\n",
        "\n",
        "    def create_frames_loader(self, video_path):\n",
        "        return self.person_detector.create_dataset(video_path)\n",
        "\n",
        "    def detect_person(self,frames):\n",
        "        for path, im, im0s, vid_cap, s in frames:\n",
        "            results = self.person_detector.model(im)\n",
        "            self.frame_list.append(results)\n",
        "            results.print()\n",
        "\n",
        "    def detect_body_parts(self):\n",
        "        imgs = []\n",
        "        for frame_i in self.frame_list:\n",
        "            crops = frame_i.crop(save=False)\n",
        "            for crop_i in crops:\n",
        "                img = crop_i['im']\n",
        "                imgs.append(img)\n",
        "                candidate, subset = self.open_pose_detector(img)\n",
        "                try:\n",
        "                    #candidate[:, 0] = candidate[:, 0]\n",
        "                    #candidate[:, 1] = candidate[:, 1]\n",
        "                    candidate = np.concatenate([candidate, [[0.0, 0.0, 0.0, 0.0]]])\n",
        "                    candidate = candidate[subset[0, :-2].astype('int'), 0:2]\n",
        "                except:\n",
        "                    candidate = np.zeros((18, 2))\n",
        "                self.body_points.append(candidate)\n",
        "                break\n",
        "\n",
        "        %matplotlib inline\n",
        "        canvas = draw_bodypose(img, candidate[:,:], subset[[0]])\n",
        "        plt.imshow(canvas[:, :, [2, 1, 0]]) \n",
        "\n",
        "    def get_actions(self):\n",
        "        self.actions = self.action_detector.recognize_actions(self.body_points)\n",
        "\n",
        "\n",
        "    def finalize(self):\n",
        "\n",
        "        if os.path.exists('output'):\n",
        "            shutil.rmtree('output')\n",
        "        os.mkdir('output')\n",
        "        h, w = self.frame_list[0].imgs[0].shape[0:2]\n",
        "        out_file = cv2.VideoWriter('out.mp4', cv2.VideoWriter_fourcc(*'DIVX'), 15, (w, h))\n",
        "        for i, (frame_i, action) in enumerate(zip(self.frame_list, self.actions)):\n",
        "            img = frame_i.imgs[0][..., ::-1]\n",
        "            bbox = frame_i.xyxy[0].tolist()[0]\n",
        "            print(i, bbox, action)\n",
        "            img = self.draw_rect(img, bbox, self.colors[action], 2, self.action_detector.action_dict[action])\n",
        "            cv2.imwrite(f'output/{i:05}.jpg', img)\n",
        "            out_file.write(img)\n",
        "        out_file.release()\n"
      ],
      "metadata": {
        "id": "WPqaFHYK39Dn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Person Detection\n",
        "\n",
        "class Frames:\n",
        "    VID_FORMATS = ['mp4', 'mov', 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mpeg', 'mpg', 'wmv']\n",
        "\n",
        "    def __init__(self, video_input, img_size=640, stride=32, auto=True):\n",
        "\n",
        "        p = str(Path(video_input).resolve())\n",
        "        if os.path.isfile(video_input):\n",
        "            files = [video_input]\n",
        "        else:\n",
        "            raise Exception(f'ERROR: {p} not found')\n",
        "\n",
        "        self.files = [x for x in files if x.split('.')[-1].lower() in self.VID_FORMATS]\n",
        "\n",
        "        self.nf = len(files)\n",
        "        self.stride = stride\n",
        "        self.new_video(files[0])\n",
        "        self.auto = auto\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def new_video(self, path):\n",
        "        self.frame = 0\n",
        "        self.capture = cv2.VideoCapture(path)\n",
        "        self.frames = int(self.capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_f\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.count == self.nf:\n",
        "            raise StopIteration\n",
        "        path = self.files[self.count]\n",
        "\n",
        "        ret_val, img0 = self.capture.read()\n",
        "        while not ret_val:\n",
        "            self.count += 1\n",
        "            self.capture.release()\n",
        "            if self.count == self.nf:  # last video\n",
        "                raise StopIteration\n",
        "            else:\n",
        "                path = self.files[self.count]\n",
        "                self.new_video(path)\n",
        "                ret_val, img0 = self.cap.read()\n",
        "\n",
        "        self.frame += 1\n",
        "        s = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}:'\n",
        "\n",
        "        # Padded resize\n",
        "        img = self.letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]\n",
        "\n",
        "        # Convert\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return path, img, img0, self.capture, s\n",
        "\n",
        "    @staticmethod\n",
        "    def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "        # Resize and pad image while meeting stride-multiple constraints\n",
        "        shape = im.shape[:2]  # current shape [height, width]\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        # Scale ratio (new / old)\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        # Compute padding\n",
        "        ratio = r, r  # width, height ratios\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if auto:  # minimum rectangle\n",
        "            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "        elif scaleFill:  # stretch\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        dw /= 2  # divide padding into 2 sides\n",
        "        dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:  # resize\n",
        "            im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "        im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "        return im, ratio, (dw, dh)\n",
        "\n",
        "\n",
        "class PersonDetection():\n",
        "    def __init__(self):\n",
        "        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "        model.classes = [0]\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_dataset(video_file):\n",
        "        video_file = str(video_file)\n",
        "        imgsz = (640, 640)\n",
        "        stride = 32\n",
        "\n",
        "        data = Frames(video_file, img_size=imgsz, stride=stride, auto=True)\n",
        "\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "Qf8mzmHE22Oj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open Pose\n",
        "\n",
        "def draw_bodypose(canvas, candidate, subset):\n",
        "    stickwidth = 4\n",
        "    limbSeq = [[2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10], \\\n",
        "               [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17], \\\n",
        "               [1, 16], [16, 18], [3, 17], [6, 18]]\n",
        "\n",
        "    colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
        "              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
        "              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
        "    for i in range(18):\n",
        "        for n in range(len(subset)):\n",
        "            index = int(subset[n][i])\n",
        "            if index == -1:\n",
        "                continue\n",
        "            x, y = candidate[index][0:2]\n",
        "            cv2.circle(canvas, (int(x), int(y)), 4, colors[i], thickness=-1)\n",
        "    for i in range(17):\n",
        "        for n in range(len(subset)):\n",
        "            index = subset[n][np.array(limbSeq[i]) - 1]\n",
        "            if -1 in index:\n",
        "                continue\n",
        "            cur_canvas = canvas.copy()\n",
        "            Y = candidate[index.astype(int), 0]\n",
        "            X = candidate[index.astype(int), 1]\n",
        "            mX = np.mean(X)\n",
        "            mY = np.mean(Y)\n",
        "            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
        "            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
        "            polygon = cv2.ellipse2Poly((int(mY), int(mX)), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n",
        "            cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n",
        "            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
        "    # plt.imsave(\"preview.jpg\", canvas[:, :, [2, 1, 0]])\n",
        "    # plt.imshow(canvas[:, :, [2, 1, 0]])\n",
        "    return canvas\n",
        "\n",
        "def padRightDownCorner(img, stride, padValue):\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "\n",
        "    pad = 4 * [None]\n",
        "    pad[0] = 0 # up\n",
        "    pad[1] = 0 # left\n",
        "    pad[2] = 0 if (h % stride == 0) else stride - (h % stride) # down\n",
        "    pad[3] = 0 if (w % stride == 0) else stride - (w % stride) # right\n",
        "\n",
        "    img_padded = img\n",
        "    pad_up = np.tile(img_padded[0:1, :, :]*0 + padValue, (pad[0], 1, 1))\n",
        "    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n",
        "    pad_left = np.tile(img_padded[:, 0:1, :]*0 + padValue, (1, pad[1], 1))\n",
        "    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n",
        "    pad_down = np.tile(img_padded[-2:-1, :, :]*0 + padValue, (pad[2], 1, 1))\n",
        "    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n",
        "    pad_right = np.tile(img_padded[:, -2:-1, :]*0 + padValue, (1, pad[3], 1))\n",
        "    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n",
        "\n",
        "    return img_padded, pad\n",
        "\n",
        "def padRightDownCorner(img, stride, padValue):\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "\n",
        "    pad = 4 * [None]\n",
        "    pad[0] = 0 # up\n",
        "    pad[1] = 0 # left\n",
        "    pad[2] = 0 if (h % stride == 0) else stride - (h % stride) # down\n",
        "    pad[3] = 0 if (w % stride == 0) else stride - (w % stride) # right\n",
        "\n",
        "    img_padded = img\n",
        "    pad_up = np.tile(img_padded[0:1, :, :]*0 + padValue, (pad[0], 1, 1))\n",
        "    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n",
        "    pad_left = np.tile(img_padded[:, 0:1, :]*0 + padValue, (1, pad[1], 1))\n",
        "    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n",
        "    pad_down = np.tile(img_padded[-2:-1, :, :]*0 + padValue, (pad[2], 1, 1))\n",
        "    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n",
        "    pad_right = np.tile(img_padded[:, -2:-1, :]*0 + padValue, (1, pad[3], 1))\n",
        "    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n",
        "\n",
        "    return img_padded, pad\n",
        "\n",
        "def transfer(model, model_weights):\n",
        "    transfered_model_weights = {}\n",
        "    for weights_name in model.state_dict().keys():\n",
        "        transfered_model_weights[weights_name] = model_weights['.'.join(weights_name.split('.')[1:])]\n",
        "    return transfered_model_weights\n",
        "\n",
        "def make_layers(block, no_relu_layers):\n",
        "    layers = []\n",
        "    for layer_name, v in block.items():\n",
        "        if 'pool' in layer_name:\n",
        "            layer = nn.MaxPool2d(kernel_size=v[0], stride=v[1],\n",
        "                                    padding=v[2])\n",
        "            layers.append((layer_name, layer))\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],\n",
        "                               kernel_size=v[2], stride=v[3],\n",
        "                               padding=v[4])\n",
        "            layers.append((layer_name, conv2d))\n",
        "            if layer_name not in no_relu_layers:\n",
        "                layers.append(('relu_'+layer_name, nn.ReLU(inplace=True)))\n",
        "\n",
        "    return nn.Sequential(OrderedDict(layers))\n",
        "\n",
        "\n",
        "class BodyPoseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BodyPoseModel, self).__init__()\n",
        "\n",
        "        # these layers have no relu layer\n",
        "        no_relu_layers = ['conv5_5_CPM_L1', 'conv5_5_CPM_L2', 'Mconv7_stage2_L1',\\\n",
        "                          'Mconv7_stage2_L2', 'Mconv7_stage3_L1', 'Mconv7_stage3_L2',\\\n",
        "                          'Mconv7_stage4_L1', 'Mconv7_stage4_L2', 'Mconv7_stage5_L1',\\\n",
        "                          'Mconv7_stage5_L2', 'Mconv7_stage6_L1', 'Mconv7_stage6_L1']\n",
        "        blocks = {}\n",
        "        block0 = OrderedDict([\n",
        "                      ('conv1_1', [3, 64, 3, 1, 1]),\n",
        "                      ('conv1_2', [64, 64, 3, 1, 1]),\n",
        "                      ('pool1_stage1', [2, 2, 0]),\n",
        "                      ('conv2_1', [64, 128, 3, 1, 1]),\n",
        "                      ('conv2_2', [128, 128, 3, 1, 1]),\n",
        "                      ('pool2_stage1', [2, 2, 0]),\n",
        "                      ('conv3_1', [128, 256, 3, 1, 1]),\n",
        "                      ('conv3_2', [256, 256, 3, 1, 1]),\n",
        "                      ('conv3_3', [256, 256, 3, 1, 1]),\n",
        "                      ('conv3_4', [256, 256, 3, 1, 1]),\n",
        "                      ('pool3_stage1', [2, 2, 0]),\n",
        "                      ('conv4_1', [256, 512, 3, 1, 1]),\n",
        "                      ('conv4_2', [512, 512, 3, 1, 1]),\n",
        "                      ('conv4_3_CPM', [512, 256, 3, 1, 1]),\n",
        "                      ('conv4_4_CPM', [256, 128, 3, 1, 1])\n",
        "                  ])\n",
        "\n",
        "\n",
        "        # Stage 1\n",
        "        block1_1 = OrderedDict([\n",
        "                        ('conv5_1_CPM_L1', [128, 128, 3, 1, 1]),\n",
        "                        ('conv5_2_CPM_L1', [128, 128, 3, 1, 1]),\n",
        "                        ('conv5_3_CPM_L1', [128, 128, 3, 1, 1]),\n",
        "                        ('conv5_4_CPM_L1', [128, 512, 1, 1, 0]),\n",
        "                        ('conv5_5_CPM_L1', [512, 38, 1, 1, 0])\n",
        "                    ])\n",
        "\n",
        "        block1_2 = OrderedDict([\n",
        "                        ('conv5_1_CPM_L2', [128, 128, 3, 1, 1]),\n",
        "                        ('conv5_2_CPM_L2', [128, 128, 3, 1, 1]),\n",
        "                        ('conv5_3_CPM_L2', [128, 128, 3, 1, 1]),\n",
        "                        ('conv5_4_CPM_L2', [128, 512, 1, 1, 0]),\n",
        "                        ('conv5_5_CPM_L2', [512, 19, 1, 1, 0])\n",
        "                    ])\n",
        "        blocks['block1_1'] = block1_1\n",
        "        blocks['block1_2'] = block1_2\n",
        "\n",
        "        self.model0 = make_layers(block0, no_relu_layers)\n",
        "\n",
        "        # Stages 2 - 6\n",
        "        for i in range(2, 7):\n",
        "            blocks['block%d_1' % i] = OrderedDict([\n",
        "                    ('Mconv1_stage%d_L1' % i, [185, 128, 7, 1, 3]),\n",
        "                    ('Mconv2_stage%d_L1' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv3_stage%d_L1' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv4_stage%d_L1' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv5_stage%d_L1' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv6_stage%d_L1' % i, [128, 128, 1, 1, 0]),\n",
        "                    ('Mconv7_stage%d_L1' % i, [128, 38, 1, 1, 0])\n",
        "                ])\n",
        "\n",
        "            blocks['block%d_2' % i] = OrderedDict([\n",
        "                    ('Mconv1_stage%d_L2' % i, [185, 128, 7, 1, 3]),\n",
        "                    ('Mconv2_stage%d_L2' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv3_stage%d_L2' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv4_stage%d_L2' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv5_stage%d_L2' % i, [128, 128, 7, 1, 3]),\n",
        "                    ('Mconv6_stage%d_L2' % i, [128, 128, 1, 1, 0]),\n",
        "                    ('Mconv7_stage%d_L2' % i, [128, 19, 1, 1, 0])\n",
        "                ])\n",
        "\n",
        "        for k in blocks.keys():\n",
        "            blocks[k] = make_layers(blocks[k], no_relu_layers)\n",
        "\n",
        "        self.model1_1 = blocks['block1_1']\n",
        "        self.model2_1 = blocks['block2_1']\n",
        "        self.model3_1 = blocks['block3_1']\n",
        "        self.model4_1 = blocks['block4_1']\n",
        "        self.model5_1 = blocks['block5_1']\n",
        "        self.model6_1 = blocks['block6_1']\n",
        "\n",
        "        self.model1_2 = blocks['block1_2']\n",
        "        self.model2_2 = blocks['block2_2']\n",
        "        self.model3_2 = blocks['block3_2']\n",
        "        self.model4_2 = blocks['block4_2']\n",
        "        self.model5_2 = blocks['block5_2']\n",
        "        self.model6_2 = blocks['block6_2']\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out1 = self.model0(x)\n",
        "\n",
        "        out1_1 = self.model1_1(out1)\n",
        "        out1_2 = self.model1_2(out1)\n",
        "        out2 = torch.cat([out1_1, out1_2, out1], 1)\n",
        "\n",
        "        out2_1 = self.model2_1(out2)\n",
        "        out2_2 = self.model2_2(out2)\n",
        "        out3 = torch.cat([out2_1, out2_2, out1], 1)\n",
        "\n",
        "        out3_1 = self.model3_1(out3)\n",
        "        out3_2 = self.model3_2(out3)\n",
        "        out4 = torch.cat([out3_1, out3_2, out1], 1)\n",
        "\n",
        "        out4_1 = self.model4_1(out4)\n",
        "        out4_2 = self.model4_2(out4)\n",
        "        out5 = torch.cat([out4_1, out4_2, out1], 1)\n",
        "\n",
        "        out5_1 = self.model5_1(out5)\n",
        "        out5_2 = self.model5_2(out5)\n",
        "        out6 = torch.cat([out5_1, out5_2, out1], 1)\n",
        "\n",
        "        out6_1 = self.model6_1(out6)\n",
        "        out6_2 = self.model6_2(out6)\n",
        "\n",
        "        return out6_1, out6_2\n",
        "\n",
        "\n",
        "class Body(object):\n",
        "    def __init__(self, model_path):\n",
        "        self.model = BodyPoseModel()\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "        model_dict = transfer(self.model, torch.load(model_path))\n",
        "        self.model.load_state_dict(model_dict)\n",
        "        self.model.eval()\n",
        "\n",
        "    def __call__(self, oriImg):\n",
        "        # scale_search = [0.5, 1.0, 1.5, 2.0]\n",
        "        scale_search = [0.5]\n",
        "        boxsize = 368\n",
        "        stride = 8\n",
        "        padValue = 128\n",
        "        thre1 = 0.1\n",
        "        thre2 = 0.05\n",
        "        multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]\n",
        "        heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n",
        "        paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n",
        "\n",
        "        for m in range(len(multiplier)):\n",
        "            scale = multiplier[m]\n",
        "            imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "            imageToTest_padded, pad = padRightDownCorner(imageToTest, stride, padValue)\n",
        "            im = np.transpose(np.float32(imageToTest_padded[:, :, :, np.newaxis]), (3, 2, 0, 1)) / 256 - 0.5\n",
        "            im = np.ascontiguousarray(im)\n",
        "\n",
        "            data = torch.from_numpy(im).float()\n",
        "            if torch.cuda.is_available():\n",
        "                data = data.cuda()\n",
        "            # data = data.permute([2, 0, 1]).unsqueeze(0).float()\n",
        "            with torch.no_grad():\n",
        "                Mconv7_stage6_L1, Mconv7_stage6_L2 = self.model(data)\n",
        "            Mconv7_stage6_L1 = Mconv7_stage6_L1.cpu().numpy()\n",
        "            Mconv7_stage6_L2 = Mconv7_stage6_L2.cpu().numpy()\n",
        "\n",
        "            # extract outputs, resize, and remove padding\n",
        "            # heatmap = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[1]].data), (1, 2, 0))  # output 1 is heatmaps\n",
        "            heatmap = np.transpose(np.squeeze(Mconv7_stage6_L2), (1, 2, 0))  # output 1 is heatmaps\n",
        "            heatmap = cv2.resize(heatmap, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n",
        "            heatmap = heatmap[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]\n",
        "            heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # paf = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[0]].data), (1, 2, 0))  # output 0 is PAFs\n",
        "            paf = np.transpose(np.squeeze(Mconv7_stage6_L1), (1, 2, 0))  # output 0 is PAFs\n",
        "            paf = cv2.resize(paf, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n",
        "            paf = paf[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]\n",
        "            paf = cv2.resize(paf, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            heatmap_avg += heatmap_avg + heatmap / len(multiplier)\n",
        "            paf_avg += + paf / len(multiplier)\n",
        "\n",
        "        all_peaks = []\n",
        "        peak_counter = 0\n",
        "\n",
        "        for part in range(18):\n",
        "            map_ori = heatmap_avg[:, :, part]\n",
        "            one_heatmap = gaussian_filter(map_ori, sigma=3)\n",
        "\n",
        "            map_left = np.zeros(one_heatmap.shape)\n",
        "            map_left[1:, :] = one_heatmap[:-1, :]\n",
        "            map_right = np.zeros(one_heatmap.shape)\n",
        "            map_right[:-1, :] = one_heatmap[1:, :]\n",
        "            map_up = np.zeros(one_heatmap.shape)\n",
        "            map_up[:, 1:] = one_heatmap[:, :-1]\n",
        "            map_down = np.zeros(one_heatmap.shape)\n",
        "            map_down[:, :-1] = one_heatmap[:, 1:]\n",
        "\n",
        "            peaks_binary = np.logical_and.reduce(\n",
        "                (one_heatmap >= map_left, one_heatmap >= map_right, one_heatmap >= map_up, one_heatmap >= map_down, one_heatmap > thre1))\n",
        "            peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]))  # note reverse\n",
        "            peaks_with_score = [x + (map_ori[x[1], x[0]],) for x in peaks]\n",
        "            peak_id = range(peak_counter, peak_counter + len(peaks))\n",
        "            peaks_with_score_and_id = [peaks_with_score[i] + (peak_id[i],) for i in range(len(peak_id))]\n",
        "\n",
        "            all_peaks.append(peaks_with_score_and_id)\n",
        "            peak_counter += len(peaks)\n",
        "\n",
        "        # find connection in the specified sequence, center 29 is in the position 15\n",
        "        limbSeq = [[2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10], \\\n",
        "                   [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17], \\\n",
        "                   [1, 16], [16, 18], [3, 17], [6, 18]]\n",
        "        # the middle joints heatmap correpondence\n",
        "        mapIdx = [[31, 32], [39, 40], [33, 34], [35, 36], [41, 42], [43, 44], [19, 20], [21, 22], \\\n",
        "                  [23, 24], [25, 26], [27, 28], [29, 30], [47, 48], [49, 50], [53, 54], [51, 52], \\\n",
        "                  [55, 56], [37, 38], [45, 46]]\n",
        "\n",
        "        connection_all = []\n",
        "        special_k = []\n",
        "        mid_num = 10\n",
        "\n",
        "        for k in range(len(mapIdx)):\n",
        "            score_mid = paf_avg[:, :, [x - 19 for x in mapIdx[k]]]\n",
        "            candA = all_peaks[limbSeq[k][0] - 1]\n",
        "            candB = all_peaks[limbSeq[k][1] - 1]\n",
        "            nA = len(candA)\n",
        "            nB = len(candB)\n",
        "            indexA, indexB = limbSeq[k]\n",
        "            if (nA != 0 and nB != 0):\n",
        "                connection_candidate = []\n",
        "                for i in range(nA):\n",
        "                    for j in range(nB):\n",
        "                        vec = np.subtract(candB[j][:2], candA[i][:2])\n",
        "                        norm = math.sqrt(vec[0] * vec[0] + vec[1] * vec[1])\n",
        "                        norm = max(0.001, norm)\n",
        "                        vec = np.divide(vec, norm)\n",
        "\n",
        "                        startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n",
        "                                            np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n",
        "\n",
        "                        vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n",
        "                                          for I in range(len(startend))])\n",
        "                        vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n",
        "                                          for I in range(len(startend))])\n",
        "\n",
        "                        score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n",
        "                        score_with_dist_prior = sum(score_midpts) / len(score_midpts) + min(\n",
        "                            0.5 * oriImg.shape[0] / norm - 1, 0)\n",
        "                        criterion1 = len(np.nonzero(score_midpts > thre2)[0]) > 0.8 * len(score_midpts)\n",
        "                        criterion2 = score_with_dist_prior > 0\n",
        "                        if criterion1 and criterion2:\n",
        "                            connection_candidate.append(\n",
        "                                [i, j, score_with_dist_prior, score_with_dist_prior + candA[i][2] + candB[j][2]])\n",
        "\n",
        "                connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n",
        "                connection = np.zeros((0, 5))\n",
        "                for c in range(len(connection_candidate)):\n",
        "                    i, j, s = connection_candidate[c][0:3]\n",
        "                    if (i not in connection[:, 3] and j not in connection[:, 4]):\n",
        "                        connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n",
        "                        if (len(connection) >= min(nA, nB)):\n",
        "                            break\n",
        "\n",
        "                connection_all.append(connection)\n",
        "            else:\n",
        "                special_k.append(k)\n",
        "                connection_all.append([])\n",
        "\n",
        "        subset = -1 * np.ones((0, 20))\n",
        "        candidate = np.array([item for sublist in all_peaks for item in sublist])\n",
        "\n",
        "        for k in range(len(mapIdx)):\n",
        "            if k not in special_k:\n",
        "                partAs = connection_all[k][:, 0]\n",
        "                partBs = connection_all[k][:, 1]\n",
        "                indexA, indexB = np.array(limbSeq[k]) - 1\n",
        "\n",
        "                for i in range(len(connection_all[k])):  # = 1:size(temp,1)\n",
        "                    found = 0\n",
        "                    subset_idx = [-1, -1]\n",
        "                    for j in range(len(subset)):  # 1:size(subset,1):\n",
        "                        if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n",
        "                            subset_idx[found] = j\n",
        "                            found += 1\n",
        "\n",
        "                    if found == 1:\n",
        "                        j = subset_idx[0]\n",
        "                        if subset[j][indexB] != partBs[i]:\n",
        "                            subset[j][indexB] = partBs[i]\n",
        "                            subset[j][-1] += 1\n",
        "                            subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
        "                    elif found == 2:  # if found 2 and disjoint, merge them\n",
        "                        j1, j2 = subset_idx\n",
        "                        membership = ((subset[j1] >= 0).astype(int) + (subset[j2] >= 0).astype(int))[:-2]\n",
        "                        if len(np.nonzero(membership == 2)[0]) == 0:  # merge\n",
        "                            subset[j1][:-2] += (subset[j2][:-2] + 1)\n",
        "                            subset[j1][-2:] += subset[j2][-2:]\n",
        "                            subset[j1][-2] += connection_all[k][i][2]\n",
        "                            subset = np.delete(subset, j2, 0)\n",
        "                        else:  # as like found == 1\n",
        "                            subset[j1][indexB] = partBs[i]\n",
        "                            subset[j1][-1] += 1\n",
        "                            subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
        "\n",
        "                    # if find no partA in the subset, create a new subset\n",
        "                    elif not found and k < 17:\n",
        "                        row = -1 * np.ones(20)\n",
        "                        row[indexA] = partAs[i]\n",
        "                        row[indexB] = partBs[i]\n",
        "                        row[-1] = 2\n",
        "                        row[-2] = sum(candidate[connection_all[k][i, :2].astype(int), 2]) + connection_all[k][i][2]\n",
        "                        subset = np.vstack([subset, row])\n",
        "        # delete some rows of subset which has few parts occur\n",
        "        deleteIdx = []\n",
        "        for i in range(len(subset)):\n",
        "            if subset[i][-1] < 4 or subset[i][-2] / subset[i][-1] < 0.4:\n",
        "                deleteIdx.append(i)\n",
        "        subset = np.delete(subset, deleteIdx, axis=0)\n",
        "\n",
        "        # subset: n*20 array, 0-17 is the index in candidate, 18 is the total score, 19 is the total parts\n",
        "        # candidate: x, y, score, id\n",
        "        return candidate, subset\n"
      ],
      "metadata": {
        "id": "0tiz2xHZ2x6d"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Action Recognition Model\n",
        "\n",
        "class ActionDetection:\n",
        "    def __init__(self, model_path):\n",
        "        self.angle_idx = [[2, 3], [3, 4], [5, 6], [6, 7], [8, 9], [9, 10], [11, 12], [12, 13]]\n",
        "        self.dist_idx = [3, 4, 6, 7, 9, 10, 12, 13]\n",
        "        self.action_dict = {0: 'Standing', 1: 'Walking', 2: 'Hitting', 3: 'Kicking'}\n",
        "        #self.model = LSTMClassifier(64,4)\n",
        "        self.model = torch.load(model_path)\n",
        "        self.model.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "\n",
        "    def calculate_features(self, body_points):\n",
        "\n",
        "        features = []\n",
        "        count = 0\n",
        "        for bp_i in body_points:\n",
        "\n",
        "            angles = [self.get_angle(bp_i[i], bp_i[j]) for i, j in self.angle_idx]\n",
        "            if count == 0:\n",
        "                bp_i_t = bp_i\n",
        "                angles_t = np.array(angles)\n",
        "\n",
        "            delta_angle = abs((np.array(angles) - (angles_t))).tolist()\n",
        "            delta_dist = [self.get_dist(bp_i_t[j], bp_i[j]) for j in self.dist_idx]\n",
        "\n",
        "            features.append(angles + delta_angle + delta_dist)\n",
        "            bp_i_t = bp_i\n",
        "            angles_t = np.array(angles)\n",
        "            count = count + 1\n",
        "\n",
        "        features = np.array(features)\n",
        "        features = np.concatenate([np.zeros(((features.shape[0] // 32 + 1) * 32 - features.shape[0], 24)), features])\n",
        "        features = np.expand_dims(features, 0)\n",
        "\n",
        "        return torch.from_numpy(features).float()\n",
        "\n",
        "    def recognize_actions(self, body_points):\n",
        "\n",
        "        data = self.calculate_features(body_points)\n",
        "        out = []\n",
        "        with torch.no_grad():\n",
        "            if torch.cuda.is_available():\n",
        "                for i in range(data.shape[1]):\n",
        "                    out.append(self.model(data[:, i:i + 32, :].cuda()).argmax(-1).cpu().numpy()[0])\n",
        "            else:\n",
        "                for i in range(data.shape[1]):\n",
        "                    out.append(self.model(data[:, i:i + 32, :]).argmax(-1).numpy()[0])\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dist(joint_1, joint_2):\n",
        "        x1, y1 = joint_1\n",
        "        x2, y2 = joint_2\n",
        "        if (x1 == 0) or (x2 == 0) or (y1 == 0):\n",
        "            return 0.0\n",
        "        else:\n",
        "            return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_angle(joint_1, joint_2):\n",
        "        x1, y1 = joint_1\n",
        "        x2, y2 = joint_2\n",
        "\n",
        "        if (x1 == 0) or (x2 == 0):\n",
        "            return 0.0\n",
        "        dx = x2 - x1\n",
        "        dy = y2 - y1\n",
        "        rad = math.atan2(dy, dx)\n",
        "        degree = (rad * 180) / math.pi\n",
        "        if (degree < 0):\n",
        "            degree = degree + 360\n",
        "        return degree\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "class ARTrainer:\n",
        "    import tensorflow as tf\n",
        "    def __init__(self, val_ratio=0.1):\n",
        "        self.val_ratio = val_ratio\n",
        "        self.train = pd.read_csv('data/pose_36.txt', index_col=None, header=None)\n",
        "        self.out = pd.read_csv('data/pose36_c.txt', index_col=None, header=None)\n",
        "\n",
        "    def create_data(self):\n",
        "        out = pd.get_dummies(self.out[0])\n",
        "        train = self.train.values\n",
        "        train = train.reshape(-1, 32, 24)\n",
        "        print(train.shape)\n",
        "        out = out.values\n",
        "\n",
        "        out = np.expand_dims(out, -1).transpose(0, 2, 1)\n",
        "        out = np.repeat(out, 32, axis=1)\n",
        "        print(out.shape)\n",
        "\n",
        "        train_inp, val_inp, train_out, val_out = train_test_split(train, out, test_size=self.val_ratio, shuffle=True)\n",
        "\n",
        "        return train_inp, val_inp, train_out, val_out\n",
        "\n",
        "    @classmethod\n",
        "    def create_tf_model(cls):\n",
        "        series_input = cls.tf.keras.layers.Input((32, 24))\n",
        "        series_input = cls.tf.keras.layers.GaussianNoise(0.05)(series_input)\n",
        "        # x = cls.tf.keras.layers.Dropout(0.25)(series_input)\n",
        "        x = cls.tf.keras.layers.LSTM(64, dropout=0.2, return_sequences=True)(series_input)\n",
        "        x = cls.tf.keras.layers.TimeDistributed(cls.tf.keras.layers.Dropout(0.2))(x)\n",
        "        x = cls.tf.keras.layers.TimeDistributed(cls.tf.keras.layers.Dense(16, activation='relu'))(x)\n",
        "        x = cls.tf.keras.layers.TimeDistributed(cls.tf.keras.layers.Dropout(0.05))(x)\n",
        "        x = cls.tf.keras.layers.TimeDistributed(cls.tf.keras.layers.Dense(4))(x)\n",
        "        x = cls.tf.keras.layers.TimeDistributed(cls.tf.keras.layers.Softmax())(x)\n",
        "\n",
        "        return cls.tf.keras.Model(series_input, x)\n",
        "\n",
        "    @classmethod\n",
        "    def compile_tf_model(cls, m):\n",
        "        optimizer = cls.tf.optimizers.Adam(learning_rate=0.001)\n",
        "        m.compile(optimizer=optimizer, loss=cls.tf.keras.losses.categorical_crossentropy,\n",
        "                  metrics=cls.tf.keras.metrics.categorical_accuracy)\n",
        "        return m\n",
        "\n",
        "    @classmethod\n",
        "    def create_tf_checkpoint(cls):\n",
        "        ckpt = cls.tf.keras.callbacks.ModelCheckpoint(\n",
        "            'model/check_weights.h5', monitor='val_categorical_accuracy', verbose=0, save_best_only=True, mode='min',\n",
        "            save_weights_only=True\n",
        "        )\n",
        "        return ckpt\n",
        "\n",
        "    def train_tf_model(self, batch_size):\n",
        "        train_inp, val_inp, train_out, val_out = self.create_data()\n",
        "        model = self.create_tf_model()\n",
        "        model = self.compile_tf_model(model)\n",
        "        checkpoints = self.create_tf_checkpoint()\n",
        "\n",
        "        model.fit(x=train_inp, y=train_out, batch_size=batch_size, epochs=250, validation_batch_size=batch_size,\n",
        "                  validation_data=(val_inp, val_out), callbacks=[checkpoints], shuffle=True)\n",
        "\n",
        "        model.save('model/classifier.h5', include_optimizer=False)\n",
        "        model.save_weights('model/classifier_w.h5')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_tensorflow_pytorh_model(self):\n",
        "        tf_model = self.train_tf_model(128)\n",
        "        weights = tf_model.get_weights()\n",
        "        pt_model = LSTMClassifier(64, 4)\n",
        "\n",
        "        pt_model.lstm.weight_ih_l0.data = torch.from_numpy(weights[0]).transpose(1, 0).contiguous()\n",
        "        pt_model.lstm.weight_hh_l0.data = torch.from_numpy(weights[1]).transpose(1, 0).contiguous()\n",
        "\n",
        "        pt_model.lstm.bias_ih_l0.data = torch.from_numpy(weights[2]).contiguous()\n",
        "        pt_model.lstm.bias_hh_l0.data = torch.from_numpy(weights[2]).contiguous()\n",
        "\n",
        "        pt_model.hidden2inter.weight.data = torch.from_numpy(weights[3]).transpose(1, 0).contiguous()\n",
        "        pt_model.hidden2inter.bias.data = torch.from_numpy(weights[4]).contiguous()\n",
        "        pt_model.hidden2out.weight.data = torch.from_numpy(weights[5]).transpose(1, 0).contiguous()\n",
        "        pt_model.hidden2out.bias.data = torch.from_numpy(weights[6]).contiguous()\n",
        "\n",
        "        torch.save(pt_model, 'model/torch_classifier.h5')\n",
        "        torch.save(pt_model.state_dict(), 'torch_state.h5')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, output_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(24, hidden_dim, num_layers=1, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.hidden2inter = nn.Linear(hidden_dim, 16)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.hidden2out = nn.Linear(16, 4)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)),\n",
        "                autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))\n",
        "\n",
        "    def forward(self, batch):\n",
        "        self.hidden = self.init_hidden(batch.size(-1))\n",
        "\n",
        "        output, (ht, ct) = self.lstm(batch)\n",
        "        # output = output.contiguous().view(-1, self.hidden_dim)\n",
        "        # ht is the last hidden state of the sequences\n",
        "        # ht = (1 x batch_size x hidden_dim)\n",
        "        # ht[-1] = (batch_size x hidden_dim)\n",
        "        output = self.hidden2inter(ht[-1])  # torch.cat((ht[0], ht[1]), dim=-1))\n",
        "        output = self.act1(output)\n",
        "        output = self.hidden2out(output)\n",
        "        output = self.softmax(output)\n",
        "        # output = output.contiguous().view(batch.size(0),-1, 4)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "dzCB6Kg62j2q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "v5BKtEuG2hZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_video = 'test.mp4' #@param {type:\"string\"} "
      ],
      "metadata": {
        "id": "7OnkSvl24bON"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_task = MainTask()\n",
        "frame_loader = main_task.create_frames_loader(input_video)\n",
        "main_task.detect_person(frame_loader)\n",
        "main_task.detect_body_parts()\n",
        "main_task.get_actions()\n",
        "main_task.finalize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCHJ9Q0s4JKU",
        "outputId": "6a6d77d5-cfa6-45ea-b963-06e86304c758"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (5.3.1)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 2022-2-7 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.1ms pre-process, 216.5ms inference, 55.4ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.8ms pre-process, 11.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 10.5ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.2ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 10.3ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.3ms pre-process, 15.3ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.3ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 14.8ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.9ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.8ms pre-process, 13.2ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 11.1ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.4ms pre-process, 12.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.2ms pre-process, 11.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 10.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.9ms pre-process, 10.7ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.1ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 10.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.6ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.5ms pre-process, 10.0ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 12.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.9ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 11.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 11.2ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.7ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.2ms pre-process, 10.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.8ms inference, 2.4ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.7ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.1ms pre-process, 11.2ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 12.2ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 15.2ms inference, 3.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.3ms pre-process, 12.2ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 18.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.2ms pre-process, 11.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 13.3ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.5ms pre-process, 10.5ms inference, 2.5ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.2ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 10.5ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.1ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.0ms pre-process, 12.3ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.9ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.6ms pre-process, 11.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 13.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.3ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 15.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.7ms pre-process, 15.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 12.5ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.6ms pre-process, 10.2ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.2ms pre-process, 11.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 10.1ms pre-process, 10.7ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.7ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.3ms pre-process, 16.4ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.5ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.5ms pre-process, 10.7ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.2ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.2ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.8ms pre-process, 11.9ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.9ms inference, 3.3ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.1ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 11.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.2ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 11.2ms inference, 3.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.0ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.0ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 11.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.9ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.1ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 11.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.5ms pre-process, 10.2ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.5ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.1ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.5ms pre-process, 10.1ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.4ms pre-process, 10.3ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 9.8ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.8ms pre-process, 13.3ms inference, 3.5ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.7ms inference, 2.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 14.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 15.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.4ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.2ms pre-process, 10.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.1ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.0ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.9ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.1ms pre-process, 9.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.7ms pre-process, 9.9ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.6ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 11.0ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.8ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.9ms pre-process, 10.2ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.3ms pre-process, 10.3ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.1ms pre-process, 10.3ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.8ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.5ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.2ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 16.1ms inference, 3.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 13.4ms inference, 2.6ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.3ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.3ms pre-process, 10.1ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.6ms pre-process, 10.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 11.0ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.8ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 11.5ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 11.0ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.0ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.3ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 10.3ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.8ms pre-process, 10.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.2ms pre-process, 11.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.6ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 11.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.6ms inference, 3.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.2ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.5ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.2ms inference, 2.6ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.3ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.8ms pre-process, 15.6ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.4ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 9.9ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 11.1ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.8ms pre-process, 15.5ms inference, 2.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.3ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.3ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.9ms pre-process, 10.2ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.0ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.8ms pre-process, 12.7ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.3ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.6ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.3ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.6ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.5ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.0ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.4ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.5ms pre-process, 13.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.5ms pre-process, 10.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.8ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.8ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.3ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.5ms pre-process, 13.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 13.4ms inference, 3.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 10.9ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 12.0ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.2ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.6ms pre-process, 11.5ms inference, 3.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.1ms inference, 1.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.0ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.5ms pre-process, 12.3ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.6ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.8ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.3ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.0ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.8ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.2ms pre-process, 11.3ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.6ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.9ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.8ms pre-process, 11.1ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.6ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 10.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 11.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 11.0ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.5ms pre-process, 11.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 11.0ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.0ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 11.0ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.9ms pre-process, 10.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 10.9ms inference, 3.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 7.0ms pre-process, 14.5ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.4ms pre-process, 10.7ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 11.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.3ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.8ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.8ms pre-process, 11.0ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.1ms pre-process, 11.4ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 16.6ms pre-process, 28.6ms inference, 2.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 10.6ms pre-process, 34.6ms inference, 8.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 5.0ms pre-process, 31.3ms inference, 6.7ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 10.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.7ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.6ms pre-process, 11.0ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.5ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.7ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.5ms pre-process, 11.2ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.5ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.4ms pre-process, 12.1ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.8ms pre-process, 10.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.1ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.6ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 16.3ms inference, 3.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 6.7ms pre-process, 31.6ms inference, 10.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.9ms pre-process, 40.1ms inference, 2.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.5ms pre-process, 19.3ms inference, 2.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.2ms pre-process, 10.5ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.4ms pre-process, 10.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.4ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 7.6ms pre-process, 11.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.6ms pre-process, 13.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.0ms pre-process, 10.5ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.0ms pre-process, 10.4ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 10.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.7ms pre-process, 10.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.4ms pre-process, 11.8ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.6ms pre-process, 10.9ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.5ms pre-process, 10.7ms inference, 1.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 12.1ms inference, 2.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 11.2ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.7ms pre-process, 10.7ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 40.2ms inference, 11.4ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 17.1ms pre-process, 29.5ms inference, 3.1ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.1ms pre-process, 15.6ms inference, 2.5ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 4.3ms pre-process, 15.3ms inference, 3.3ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 3.4ms pre-process, 12.7ms inference, 1.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 384x640 1 person\n",
            "Speed: 2.3ms pre-process, 11.8ms inference, 2.8ms NMS per image at shape (1, 3, 384, 640)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [255.2775115966797, 60.52099609375, 346.604736328125, 352.9250183105469, 0.930297315120697, 0.0] 0\n",
            "1 [252.4505615234375, 59.3040771484375, 345.87493896484375, 351.39886474609375, 0.9268267154693604, 0.0] 0\n",
            "2 [250.57765197753906, 61.230224609375, 344.3291931152344, 333.2081604003906, 0.9007244110107422, 0.0] 0\n",
            "3 [248.7115936279297, 59.08341979980469, 343.9640197753906, 339.8978271484375, 0.8930345177650452, 0.0] 0\n",
            "4 [243.67288208007812, 59.41862487792969, 338.6297912597656, 342.77728271484375, 0.9089643955230713, 0.0] 0\n",
            "5 [233.47384643554688, 59.323577880859375, 336.5400085449219, 339.9755859375, 0.8944798707962036, 0.0] 0\n",
            "6 [225.97442626953125, 58.965545654296875, 335.42303466796875, 334.9757385253906, 0.9159322381019592, 0.0] 0\n",
            "7 [219.1385040283203, 58.004058837890625, 332.43115234375, 331.919921875, 0.9258459210395813, 0.0] 0\n",
            "8 [212.07632446289062, 58.149078369140625, 327.4214782714844, 319.8216857910156, 0.934841513633728, 0.0] 0\n",
            "9 [207.37489318847656, 57.31135559082031, 326.0720520019531, 314.6497802734375, 0.9322251677513123, 0.0] 0\n",
            "10 [206.8658905029297, 57.60791015625, 324.65679931640625, 314.56854248046875, 0.9357147216796875, 0.0] 0\n",
            "11 [207.39447021484375, 58.0986328125, 322.316650390625, 314.50909423828125, 0.9327911138534546, 0.0] 0\n",
            "12 [208.9833526611328, 57.52824401855469, 318.2215881347656, 315.34124755859375, 0.9320791363716125, 0.0] 0\n",
            "13 [208.59524536132812, 59.21965026855469, 316.9222106933594, 314.8502197265625, 0.9269240498542786, 0.0] 0\n",
            "14 [208.25621032714844, 58.935997009277344, 313.54901123046875, 314.7192687988281, 0.9243432283401489, 0.0] 0\n",
            "15 [208.16104125976562, 59.8150634765625, 312.8192443847656, 314.9156494140625, 0.9256157279014587, 0.0] 0\n",
            "16 [209.04364013671875, 57.99342346191406, 311.64581298828125, 316.83349609375, 0.9179479479789734, 0.0] 0\n",
            "17 [209.81280517578125, 57.4444580078125, 311.30096435546875, 316.60430908203125, 0.9273794889450073, 0.0] 0\n",
            "18 [210.82528686523438, 57.401275634765625, 311.3447570800781, 316.8455810546875, 0.9306383728981018, 0.0] 0\n",
            "19 [212.11846923828125, 57.56854248046875, 311.92132568359375, 316.72369384765625, 0.9309900403022766, 0.0] 2\n",
            "20 [213.09027099609375, 58.55998229980469, 313.63677978515625, 316.8818359375, 0.928865373134613, 0.0] 2\n",
            "21 [213.42437744140625, 58.248992919921875, 315.22711181640625, 318.0222473144531, 0.9283366799354553, 0.0] 2\n",
            "22 [214.37403869628906, 58.896942138671875, 315.9940185546875, 318.54351806640625, 0.9276774525642395, 0.0] 2\n",
            "23 [214.49160766601562, 58.674591064453125, 321.0592346191406, 318.6313171386719, 0.9336504936218262, 0.0] 2\n",
            "24 [215.6129150390625, 59.78950500488281, 338.804931640625, 321.9251708984375, 0.9273712038993835, 0.0] 2\n",
            "25 [215.72471618652344, 60.7635498046875, 343.74859619140625, 321.2904052734375, 0.9272782206535339, 0.0] 2\n",
            "26 [215.96636962890625, 61.369049072265625, 345.147216796875, 321.0500183105469, 0.925685465335846, 0.0] 3\n",
            "27 [216.99163818359375, 61.74713134765625, 344.897705078125, 319.6912841796875, 0.9285253882408142, 0.0] 2\n",
            "28 [217.92550659179688, 63.232635498046875, 350.7005310058594, 321.69171142578125, 0.909378707408905, 0.0] 2\n",
            "29 [219.07102966308594, 66.76030731201172, 352.80029296875, 320.0982360839844, 0.8368849754333496, 0.0] 2\n",
            "30 [223.03744506835938, 66.23597717285156, 360.8275451660156, 314.054443359375, 0.8850262761116028, 0.0] 2\n",
            "31 [225.84988403320312, 65.5522689819336, 360.9901428222656, 314.6781921386719, 0.9199501276016235, 0.0] 2\n",
            "32 [233.72708129882812, 64.8927230834961, 356.7739562988281, 314.19970703125, 0.921923816204071, 0.0] 2\n",
            "33 [236.44039916992188, 64.71092224121094, 350.5907287597656, 314.01690673828125, 0.9149442911148071, 0.0] 2\n",
            "34 [235.6495361328125, 63.784423828125, 361.67156982421875, 319.3110046386719, 0.9281666278839111, 0.0] 2\n",
            "35 [235.39849853515625, 64.32464599609375, 382.06915283203125, 318.33612060546875, 0.9242502450942993, 0.0] 2\n",
            "36 [235.95703125, 65.54134368896484, 411.95501708984375, 315.26971435546875, 0.8435338735580444, 0.0] 2\n",
            "37 [235.52481079101562, 65.14379119873047, 410.7190246582031, 315.3092956542969, 0.823760986328125, 0.0] 2\n",
            "38 [232.07623291015625, 69.39078521728516, 390.6240234375, 318.79534912109375, 0.8374035954475403, 0.0] 2\n",
            "39 [234.2696533203125, 64.28046417236328, 390.0223388671875, 318.3508605957031, 0.9174500107765198, 0.0] 2\n",
            "40 [233.49473571777344, 61.74327850341797, 355.0184631347656, 315.1458435058594, 0.9337921738624573, 0.0] 2\n",
            "41 [233.63876342773438, 62.1949462890625, 350.9543151855469, 314.073486328125, 0.9179945588111877, 0.0] 2\n",
            "42 [233.6248779296875, 61.55481719970703, 355.18157958984375, 312.95538330078125, 0.929333508014679, 0.0] 2\n",
            "43 [233.20977783203125, 59.70274353027344, 361.7701416015625, 313.73651123046875, 0.936387836933136, 0.0] 2\n",
            "44 [231.27874755859375, 58.52983093261719, 363.6204833984375, 313.396728515625, 0.9334540367126465, 0.0] 2\n",
            "45 [232.27914428710938, 60.96930694580078, 353.4507751464844, 312.68133544921875, 0.9304223656654358, 0.0] 2\n",
            "46 [232.1645965576172, 60.48589324951172, 352.68023681640625, 312.3379211425781, 0.9328853487968445, 0.0] 2\n",
            "47 [232.35293579101562, 58.50458526611328, 351.0386047363281, 314.26055908203125, 0.9268901944160461, 0.0] 2\n",
            "48 [231.6681671142578, 56.0775146484375, 347.4356689453125, 314.677978515625, 0.9297592639923096, 0.0] 2\n",
            "49 [231.64651489257812, 55.620086669921875, 346.1005554199219, 314.90081787109375, 0.9332189559936523, 0.0] 2\n",
            "50 [231.96719360351562, 56.15788269042969, 344.4319763183594, 314.88702392578125, 0.9357782006263733, 0.0] 2\n",
            "51 [232.15963745117188, 56.13883972167969, 343.0616149902344, 314.76458740234375, 0.9354482293128967, 0.0] 2\n",
            "52 [232.53680419921875, 56.51145935058594, 343.256103515625, 316.19317626953125, 0.9353799819946289, 0.0] 2\n",
            "53 [232.2884979248047, 56.2188720703125, 345.1781311035156, 315.26678466796875, 0.9364093542098999, 0.0] 2\n",
            "54 [232.21426391601562, 57.104217529296875, 349.2630310058594, 315.7179260253906, 0.929400622844696, 0.0] 2\n",
            "55 [230.95388793945312, 57.69212341308594, 351.8075256347656, 316.5079345703125, 0.928398609161377, 0.0] 2\n",
            "56 [229.94784545898438, 55.96484375, 358.6512145996094, 318.549072265625, 0.9398390054702759, 0.0] 2\n",
            "57 [229.84201049804688, 55.30522155761719, 361.7295227050781, 318.69561767578125, 0.9470609426498413, 0.0] 2\n",
            "58 [230.16189575195312, 55.69139099121094, 364.0817565917969, 318.6248779296875, 0.9449677467346191, 0.0] 2\n",
            "59 [228.7912139892578, 54.55126953125, 368.25048828125, 319.8477478027344, 0.9494349956512451, 0.0] 2\n",
            "60 [234.241455078125, 53.979888916015625, 378.81591796875, 320.4931335449219, 0.9327531456947327, 0.0] 2\n",
            "61 [244.31539916992188, 53.48004150390625, 384.2663879394531, 317.035888671875, 0.9249052405357361, 0.0] 1\n",
            "62 [251.7284698486328, 50.30296325683594, 391.201904296875, 315.91534423828125, 0.9223456978797913, 0.0] 3\n",
            "63 [255.9661865234375, 48.652801513671875, 399.26043701171875, 318.13800048828125, 0.93272465467453, 0.0] 3\n",
            "64 [259.83843994140625, 47.70582580566406, 411.78814697265625, 339.5419921875, 0.9015614986419678, 0.0] 1\n",
            "65 [256.0758056640625, 49.380462646484375, 422.50653076171875, 344.7090148925781, 0.8899753093719482, 0.0] 3\n",
            "66 [257.3771057128906, 53.05747985839844, 414.7665100097656, 351.37139892578125, 0.9113200306892395, 0.0] 3\n",
            "67 [256.8641662597656, 57.494354248046875, 425.1136169433594, 344.6504821777344, 0.9334334135055542, 0.0] 3\n",
            "68 [255.1665802001953, 67.67485046386719, 422.8404541015625, 343.37408447265625, 0.916528582572937, 0.0] 3\n",
            "69 [254.86233520507812, 69.02549743652344, 411.4258728027344, 341.0787353515625, 0.9163161516189575, 0.0] 3\n",
            "70 [254.16986083984375, 68.60591125488281, 411.5855712890625, 343.7789306640625, 0.8821604251861572, 0.0] 3\n",
            "71 [274.4344177246094, 70.89048767089844, 410.4390563964844, 327.851806640625, 0.8223214745521545, 0.0] 3\n",
            "72 [305.03350830078125, 66.18701171875, 421.0723876953125, 320.635986328125, 0.8881279826164246, 0.0] 3\n",
            "73 [302.46759033203125, 66.404052734375, 445.346923828125, 312.5426025390625, 0.8636451363563538, 0.0] 3\n",
            "74 [306.2333068847656, 64.96812438964844, 468.6317443847656, 312.68463134765625, 0.9097436666488647, 0.0] 3\n",
            "75 [310.901611328125, 64.97488403320312, 474.86846923828125, 314.56494140625, 0.9147952198982239, 0.0] 3\n",
            "76 [324.12060546875, 68.51817321777344, 512.4324340820312, 312.14019775390625, 0.7526307106018066, 0.0] 3\n",
            "77 [326.40167236328125, 68.40067291259766, 518.0424194335938, 312.8327941894531, 0.8275843262672424, 0.0] 3\n",
            "78 [325.04913330078125, 64.83771514892578, 510.10150146484375, 313.8747253417969, 0.7474117875099182, 0.0] 3\n",
            "79 [325.0880126953125, 69.80042266845703, 501.204833984375, 316.555419921875, 0.8807112574577332, 0.0] 3\n",
            "80 [330.10015869140625, 72.16001892089844, 479.18853759765625, 317.30078125, 0.9221400618553162, 0.0] 3\n",
            "81 [331.942138671875, 73.19468688964844, 462.88873291015625, 317.01434326171875, 0.9278584718704224, 0.0] 3\n",
            "82 [332.034912109375, 68.95976257324219, 443.3863525390625, 317.38714599609375, 0.9025850296020508, 0.0] 3\n",
            "83 [331.36962890625, 68.72550964355469, 439.09326171875, 318.47119140625, 0.8492872714996338, 0.0] 3\n",
            "84 [326.90118408203125, 72.6971664428711, 434.71728515625, 319.98089599609375, 0.853997528553009, 0.0] 3\n",
            "85 [325.756103515625, 72.40933990478516, 433.20245361328125, 319.898193359375, 0.8687874674797058, 0.0] 3\n",
            "86 [330.209716796875, 74.2636489868164, 432.25, 317.3948669433594, 0.8753902316093445, 0.0] 3\n",
            "87 [331.178466796875, 76.80015563964844, 430.69158935546875, 312.03173828125, 0.8597854375839233, 0.0] 3\n",
            "88 [331.96441650390625, 68.43737030029297, 428.04095458984375, 316.6363525390625, 0.8602033853530884, 0.0] 3\n",
            "89 [326.6338195800781, 71.10320281982422, 426.9413146972656, 314.4421081542969, 0.902531623840332, 0.0] 3\n",
            "90 [324.245361328125, 70.76760864257812, 423.1982421875, 312.43182373046875, 0.9126138091087341, 0.0] 3\n",
            "91 [324.8580322265625, 71.90036010742188, 423.21759033203125, 307.0317687988281, 0.9191484451293945, 0.0] 3\n",
            "92 [331.3759460449219, 68.84341430664062, 422.2348937988281, 308.86663818359375, 0.9265820980072021, 0.0] 3\n",
            "93 [336.4037170410156, 67.7623062133789, 423.4692687988281, 309.11480712890625, 0.9220959544181824, 0.0] 3\n",
            "94 [338.6055603027344, 64.23164367675781, 424.7805480957031, 309.032470703125, 0.923768162727356, 0.0] 3\n",
            "95 [340.87005615234375, 60.04522705078125, 423.0703125, 311.946533203125, 0.9157285094261169, 0.0] 3\n",
            "96 [343.3155212402344, 58.14068603515625, 426.3897399902344, 313.7122497558594, 0.9195737242698669, 0.0] 3\n",
            "97 [344.11737060546875, 58.17430114746094, 428.31085205078125, 314.46820068359375, 0.91580730676651, 0.0] 3\n",
            "98 [344.2780456542969, 58.37934112548828, 431.0240783691406, 313.7657775878906, 0.9077361822128296, 0.0] 3\n",
            "99 [341.2759094238281, 60.82489013671875, 431.3999938964844, 310.5214538574219, 0.9232068061828613, 0.0] 3\n",
            "100 [345.3175964355469, 54.71623992919922, 438.2840881347656, 308.8157653808594, 0.9398126602172852, 0.0] 3\n",
            "101 [344.9779052734375, 54.38671112060547, 440.84765625, 308.7536926269531, 0.9367685317993164, 0.0] 3\n",
            "102 [348.3413391113281, 53.505096435546875, 442.5553283691406, 307.88677978515625, 0.9197836518287659, 0.0] 3\n",
            "103 [348.9543151855469, 57.938621520996094, 443.6969909667969, 311.9344787597656, 0.9101280570030212, 0.0] 3\n",
            "104 [363.9617919921875, 53.7789306640625, 444.92510986328125, 310.5628662109375, 0.907652735710144, 0.0] 3\n",
            "105 [367.0104675292969, 54.7716064453125, 447.3127136230469, 308.7934875488281, 0.921614408493042, 0.0] 3\n",
            "106 [370.568603515625, 56.299407958984375, 448.18060302734375, 308.7151794433594, 0.920623242855072, 0.0] 3\n",
            "107 [372.26605224609375, 55.862388610839844, 449.987060546875, 309.6123046875, 0.9294949173927307, 0.0] 3\n",
            "108 [374.890625, 54.86027526855469, 451.5390625, 312.5762939453125, 0.9161396622657776, 0.0] 2\n",
            "109 [375.3695373535156, 54.13005065917969, 452.0545349121094, 313.1480712890625, 0.9133529663085938, 0.0] 2\n",
            "110 [375.92681884765625, 57.520606994628906, 452.65460205078125, 309.44866943359375, 0.9154465198516846, 0.0] 2\n",
            "111 [376.3445739746094, 56.848297119140625, 453.0393981933594, 309.9598693847656, 0.9178044199943542, 0.0] 2\n",
            "112 [377.98248291015625, 57.02391815185547, 453.83282470703125, 311.008056640625, 0.9230130910873413, 0.0] 2\n",
            "113 [378.50396728515625, 58.34309387207031, 453.75543212890625, 310.9090576171875, 0.9215742945671082, 0.0] 2\n",
            "114 [378.7131652832031, 59.245811462402344, 453.8144836425781, 310.593994140625, 0.9251575469970703, 0.0] 2\n",
            "115 [379.9399719238281, 60.4576416015625, 454.1961975097656, 310.1796875, 0.926778256893158, 0.0] 2\n",
            "116 [380.655517578125, 61.787322998046875, 454.48785400390625, 310.0670166015625, 0.9236542582511902, 0.0] 2\n",
            "117 [381.260986328125, 62.003883361816406, 454.721435546875, 310.1298522949219, 0.9206368327140808, 0.0] 2\n",
            "118 [382.2964782714844, 55.657745361328125, 453.7935485839844, 312.8900451660156, 0.9271070957183838, 0.0] 2\n",
            "119 [382.5205078125, 56.57106018066406, 453.97320556640625, 312.78302001953125, 0.9256499409675598, 0.0] 2\n",
            "120 [382.18560791015625, 56.436767578125, 454.40960693359375, 314.25604248046875, 0.9278185367584229, 0.0] 2\n",
            "121 [382.0821228027344, 55.67156982421875, 454.3219299316406, 313.8807678222656, 0.9282817840576172, 0.0] 2\n",
            "122 [381.84991455078125, 55.746551513671875, 454.3150634765625, 313.7140197753906, 0.9253159761428833, 0.0] 2\n",
            "123 [380.98590087890625, 60.774620056152344, 456.2427978515625, 310.0660095214844, 0.9228314161300659, 0.0] 2\n",
            "124 [380.0042724609375, 60.3253173828125, 456.11810302734375, 309.95050048828125, 0.9247205853462219, 0.0] 2\n",
            "125 [380.1109313964844, 60.26984405517578, 456.1319885253906, 309.7723388671875, 0.923834502696991, 0.0] 2\n",
            "126 [380.02703857421875, 60.035552978515625, 455.8558349609375, 309.7920837402344, 0.9225285649299622, 0.0] 2\n",
            "127 [379.7830505371094, 60.616432189941406, 455.5803527832031, 309.3645935058594, 0.9225711226463318, 0.0] 2\n",
            "128 [379.9135437011719, 61.28434753417969, 455.1504821777344, 309.0059814453125, 0.925514817237854, 0.0] 2\n",
            "129 [379.4897155761719, 61.70086669921875, 455.1378479003906, 309.4393005371094, 0.9255832433700562, 0.0] 2\n",
            "130 [378.8489990234375, 61.89710998535156, 455.022705078125, 309.25909423828125, 0.9243461489677429, 0.0] 2\n",
            "131 [378.3861999511719, 62.24755096435547, 454.8713684082031, 309.5960693359375, 0.9236645698547363, 0.0] 2\n",
            "132 [377.3819274902344, 62.629173278808594, 454.8121032714844, 311.2391052246094, 0.9201116561889648, 0.0] 2\n",
            "133 [377.0865173339844, 62.76573181152344, 454.7423400878906, 310.89752197265625, 0.921588659286499, 0.0] 2\n",
            "134 [377.1662292480469, 63.14818572998047, 454.8072814941406, 310.79364013671875, 0.9168451428413391, 0.0] 2\n",
            "135 [376.0783996582031, 56.796226501464844, 454.0849914550781, 312.3006896972656, 0.9188233613967896, 0.0] 2\n",
            "136 [375.376953125, 56.41590881347656, 453.70703125, 313.00592041015625, 0.9202849864959717, 0.0] 2\n",
            "137 [374.8214111328125, 55.808990478515625, 453.3721923828125, 314.85064697265625, 0.9180749654769897, 0.0] 2\n",
            "138 [371.6285095214844, 55.31425476074219, 453.2225646972656, 318.07208251953125, 0.9220854043960571, 0.0] 2\n",
            "139 [368.61114501953125, 56.40788269042969, 453.4747314453125, 318.56982421875, 0.9268102049827576, 0.0] 2\n",
            "140 [359.852294921875, 61.61236572265625, 451.42840576171875, 320.8417663574219, 0.8870629668235779, 0.0] 2\n",
            "141 [358.91510009765625, 59.87644958496094, 451.035888671875, 324.802734375, 0.8936124444007874, 0.0] 2\n",
            "142 [358.3922119140625, 59.86724853515625, 450.13604736328125, 326.1391906738281, 0.894869327545166, 0.0] 2\n",
            "143 [357.12548828125, 59.47645568847656, 449.101318359375, 326.21160888671875, 0.89320307970047, 0.0] 2\n",
            "144 [360.0967102050781, 54.56782531738281, 445.3023986816406, 325.62384033203125, 0.9093560576438904, 0.0] 2\n",
            "145 [358.1313781738281, 52.6539306640625, 444.1019592285156, 326.2599182128906, 0.9085509777069092, 0.0] 2\n",
            "146 [355.06158447265625, 50.788482666015625, 442.88720703125, 326.31158447265625, 0.9160189032554626, 0.0] 2\n",
            "147 [349.3221130371094, 50.92694091796875, 440.7264709472656, 325.5494689941406, 0.9107776284217834, 0.0] 2\n",
            "148 [332.95587158203125, 50.98210144042969, 437.9791259765625, 325.5115966796875, 0.8861656785011292, 0.0] 2\n",
            "149 [332.0152893066406, 50.79087829589844, 437.3341979980469, 325.042236328125, 0.8881848454475403, 0.0] 2\n",
            "150 [334.19647216796875, 49.814300537109375, 436.29852294921875, 325.3933410644531, 0.8984457850456238, 0.0] 2\n",
            "151 [336.4070129394531, 48.592193603515625, 436.6674499511719, 326.73272705078125, 0.9058368802070618, 0.0] 2\n",
            "152 [339.57061767578125, 44.432098388671875, 437.46124267578125, 326.4394836425781, 0.9216843843460083, 0.0] 2\n",
            "153 [341.2951354980469, 42.504669189453125, 437.6549377441406, 326.0478210449219, 0.9222527146339417, 0.0] 2\n",
            "154 [343.91644287109375, 42.18910217285156, 438.20758056640625, 325.5955810546875, 0.9216818809509277, 0.0] 2\n",
            "155 [346.4767761230469, 41.16691589355469, 438.1992492675781, 325.54315185546875, 0.9206923246383667, 0.0] 2\n",
            "156 [346.32843017578125, 40.09455871582031, 438.01416015625, 325.951904296875, 0.9209455847740173, 0.0] 2\n",
            "157 [346.6026611328125, 39.547454833984375, 437.9473876953125, 325.7234802246094, 0.9209436178207397, 0.0] 2\n",
            "158 [347.255126953125, 39.615692138671875, 437.72607421875, 325.7723083496094, 0.9214118719100952, 0.0] 2\n",
            "159 [346.93731689453125, 39.548065185546875, 437.46917724609375, 326.08856201171875, 0.926652729511261, 0.0] 2\n",
            "160 [344.56005859375, 39.06256103515625, 437.22027587890625, 326.43060302734375, 0.928339421749115, 0.0] 2\n",
            "161 [344.30096435546875, 39.243896484375, 436.57489013671875, 326.6361083984375, 0.9281989932060242, 0.0] 2\n",
            "162 [343.0970764160156, 39.590606689453125, 436.4835510253906, 326.4909362792969, 0.9271880984306335, 0.0] 3\n",
            "163 [341.8841857910156, 40.28558349609375, 435.9092712402344, 326.2325134277344, 0.9239587783813477, 0.0] 3\n",
            "164 [339.3747863769531, 40.36712646484375, 434.4176940917969, 326.9300537109375, 0.9158318638801575, 0.0] 3\n",
            "165 [338.6304931640625, 40.65589904785156, 434.17645263671875, 327.58349609375, 0.9148532152175903, 0.0] 2\n",
            "166 [337.9373474121094, 40.80857849121094, 434.1770324707031, 327.4569091796875, 0.9124414920806885, 0.0] 3\n",
            "167 [337.4618835449219, 41.34979248046875, 433.8027038574219, 327.63214111328125, 0.913870632648468, 0.0] 3\n",
            "168 [337.54974365234375, 41.72467041015625, 433.191162109375, 327.8313903808594, 0.9138339161872864, 0.0] 2\n",
            "169 [337.7881164550781, 41.917449951171875, 432.8399353027344, 327.15631103515625, 0.9170554280281067, 0.0] 2\n",
            "170 [339.00616455078125, 42.06828308105469, 432.89208984375, 327.14093017578125, 0.9172379970550537, 0.0] 2\n",
            "171 [340.895263671875, 42.566314697265625, 432.78753662109375, 327.0046691894531, 0.9165331125259399, 0.0] 2\n",
            "172 [343.683349609375, 44.103302001953125, 432.69146728515625, 326.921142578125, 0.9167464971542358, 0.0] 2\n",
            "173 [345.1878356933594, 43.7109375, 433.1108703613281, 327.29180908203125, 0.9200200438499451, 0.0] 2\n",
            "174 [347.1464538574219, 44.56419372558594, 433.4494323730469, 327.35321044921875, 0.9224604964256287, 0.0] 2\n",
            "175 [349.00250244140625, 45.855499267578125, 433.91949462890625, 326.7204284667969, 0.9209440350532532, 0.0] 2\n",
            "176 [352.2213134765625, 47.7293701171875, 435.7005615234375, 326.89532470703125, 0.9214185476303101, 0.0] 2\n",
            "177 [352.7667236328125, 47.63847351074219, 436.14874267578125, 326.80029296875, 0.915372908115387, 0.0] 2\n",
            "178 [354.0112609863281, 47.7236328125, 436.8512268066406, 327.18328857421875, 0.9150850176811218, 0.0] 2\n",
            "179 [354.6974792480469, 47.89649963378906, 437.4925231933594, 326.81024169921875, 0.9147440195083618, 0.0] 2\n",
            "180 [355.6473693847656, 48.584564208984375, 438.8233337402344, 326.40185546875, 0.9123925566673279, 0.0] 2\n",
            "181 [356.1267395019531, 48.970672607421875, 439.0295104980469, 326.07470703125, 0.9135078191757202, 0.0] 2\n",
            "182 [357.18463134765625, 49.259765625, 439.41387939453125, 325.98504638671875, 0.9120072722434998, 0.0] 2\n",
            "183 [358.584716796875, 49.837677001953125, 439.43145751953125, 326.0400085449219, 0.9099394679069519, 0.0] 2\n",
            "184 [357.79180908203125, 51.141937255859375, 439.615478515625, 325.6590270996094, 0.9075613617897034, 0.0] 2\n",
            "185 [354.2376403808594, 51.456085205078125, 439.8399963378906, 325.3418273925781, 0.9076278805732727, 0.0] 2\n",
            "186 [350.12139892578125, 51.55345153808594, 439.96527099609375, 324.74859619140625, 0.911204993724823, 0.0] 2\n",
            "187 [351.1371765136719, 52.69602966308594, 440.1387634277344, 328.41156005859375, 0.8971583247184753, 0.0] 2\n",
            "188 [330.54156494140625, 53.69322204589844, 435.92950439453125, 326.12908935546875, 0.8198899626731873, 0.0] 2\n",
            "189 [328.7566833496094, 53.24833679199219, 435.1426086425781, 328.3359375, 0.8418955206871033, 0.0] 2\n",
            "190 [324.76715087890625, 51.197174072265625, 434.8797607421875, 329.2870178222656, 0.8739302754402161, 0.0] 2\n",
            "191 [318.152587890625, 51.30134582519531, 434.83660888671875, 334.4775390625, 0.8881839513778687, 0.0] 2\n",
            "192 [318.7380065917969, 54.691131591796875, 433.7201232910156, 333.5589294433594, 0.9247497916221619, 0.0] 2\n",
            "193 [307.390869140625, 55.228729248046875, 434.6126708984375, 331.9519958496094, 0.9313235878944397, 0.0] 2\n",
            "194 [287.198486328125, 51.16957092285156, 433.90130615234375, 332.66607666015625, 0.9238185286521912, 0.0] 2\n",
            "195 [280.5556640625, 49.850860595703125, 432.56610107421875, 333.2718811035156, 0.9082784652709961, 0.0] 2\n",
            "196 [288.24993896484375, 50.3355712890625, 429.38836669921875, 334.73187255859375, 0.9192880392074585, 0.0] 2\n",
            "197 [302.1834716796875, 50.431365966796875, 428.98095703125, 334.888916015625, 0.92903733253479, 0.0] 2\n",
            "198 [311.7576599121094, 49.878570556640625, 426.8263854980469, 333.97308349609375, 0.9211625456809998, 0.0] 2\n",
            "199 [319.81866455078125, 47.681732177734375, 426.3519287109375, 332.1880187988281, 0.9068143963813782, 0.0] 2\n",
            "200 [325.4189758300781, 46.30036926269531, 425.6800842285156, 332.57745361328125, 0.8951570987701416, 0.0] 2\n",
            "201 [314.6131286621094, 46.19261169433594, 426.3539123535156, 334.4901123046875, 0.9136033654212952, 0.0] 1\n",
            "202 [296.9779052734375, 46.38270568847656, 427.98944091796875, 332.53118896484375, 0.915615975856781, 0.0] 1\n",
            "203 [282.0723876953125, 46.634613037109375, 430.796875, 330.870361328125, 0.9147902727127075, 0.0] 2\n",
            "204 [279.5732116699219, 47.75730895996094, 431.9641418457031, 330.384521484375, 0.9110121130943298, 0.0] 2\n",
            "205 [289.5068664550781, 47.90010070800781, 431.5713806152344, 331.13177490234375, 0.9220529198646545, 0.0] 2\n",
            "206 [304.1653747558594, 47.89225769042969, 430.8940734863281, 330.0972900390625, 0.9197461605072021, 0.0] 2\n",
            "207 [321.92730712890625, 47.9454345703125, 431.703857421875, 334.73272705078125, 0.9175252318382263, 0.0] 2\n",
            "208 [322.424072265625, 49.87248229980469, 432.89093017578125, 334.2200927734375, 0.915130078792572, 0.0] 2\n",
            "209 [318.67498779296875, 50.33900451660156, 432.6651611328125, 332.777099609375, 0.9243295788764954, 0.0] 2\n",
            "210 [312.5517578125, 49.64332580566406, 432.58514404296875, 331.93994140625, 0.9272446632385254, 0.0] 2\n",
            "211 [312.98687744140625, 49.02229309082031, 433.4996337890625, 331.972900390625, 0.9248968958854675, 0.0] 2\n",
            "212 [322.282470703125, 47.88624572753906, 433.3026123046875, 333.06842041015625, 0.9196978807449341, 0.0] 2\n",
            "213 [323.8935852050781, 48.6123046875, 433.5214538574219, 330.7115478515625, 0.9126130938529968, 0.0] 2\n",
            "214 [327.6819152832031, 49.188385009765625, 433.8908386230469, 330.0395812988281, 0.8872186541557312, 0.0] 2\n",
            "215 [329.6366882324219, 48.32586669921875, 434.1020812988281, 329.95697021484375, 0.890393853187561, 0.0] 1\n",
            "216 [328.6756591796875, 46.503021240234375, 435.4871826171875, 332.2246398925781, 0.8739272952079773, 0.0] 2\n",
            "217 [328.2969055175781, 47.35920715332031, 435.9075622558594, 331.59857177734375, 0.8727549314498901, 0.0] 2\n",
            "218 [327.9840087890625, 48.008209228515625, 435.95098876953125, 330.9354553222656, 0.867139458656311, 0.0] 2\n",
            "219 [331.2900390625, 49.22119140625, 435.4588623046875, 330.35858154296875, 0.8398922681808472, 0.0] 2\n",
            "220 [332.63470458984375, 51.21876525878906, 435.68170166015625, 327.330322265625, 0.8228057026863098, 0.0] 2\n",
            "221 [332.0011291503906, 51.075958251953125, 435.4430236816406, 325.4561462402344, 0.8614051938056946, 0.0] 2\n",
            "222 [331.51611328125, 50.93878173828125, 436.11871337890625, 323.6080017089844, 0.8908060789108276, 0.0] 2\n",
            "223 [330.6094665527344, 51.58349609375, 435.1500549316406, 320.5826416015625, 0.9065275192260742, 0.0] 2\n",
            "224 [325.30926513671875, 52.31407165527344, 437.58612060546875, 321.92620849609375, 0.904433012008667, 0.0] 2\n",
            "225 [322.4164733886719, 53.431732177734375, 438.2316589355469, 323.9277038574219, 0.9206870198249817, 0.0] 2\n",
            "226 [320.79193115234375, 54.021209716796875, 437.03411865234375, 323.70208740234375, 0.9163732528686523, 0.0] 2\n",
            "227 [322.28045654296875, 49.92500305175781, 430.3876953125, 321.439208984375, 0.9203609824180603, 0.0] 1\n",
            "228 [314.5885009765625, 45.24371337890625, 422.3836669921875, 322.2093200683594, 0.928180456161499, 0.0] 1\n",
            "229 [305.9640808105469, 47.01103210449219, 418.6413269042969, 325.7156982421875, 0.9304846525192261, 0.0] 1\n",
            "230 [301.6833190917969, 42.10943603515625, 415.3362121582031, 324.96600341796875, 0.9201345443725586, 0.0] 2\n",
            "231 [296.47857666015625, 36.005157470703125, 409.35247802734375, 321.00152587890625, 0.9175676107406616, 0.0] 2\n",
            "232 [275.9595031738281, 36.38688659667969, 399.8587951660156, 320.63958740234375, 0.9233540892601013, 0.0] 2\n",
            "233 [269.15240478515625, 38.354644775390625, 402.8011474609375, 316.8273010253906, 0.9085647463798523, 0.0] 2\n",
            "234 [275.12347412109375, 47.15812683105469, 406.5516357421875, 318.2833251953125, 0.8802716135978699, 0.0] 2\n",
            "235 [284.17669677734375, 51.079315185546875, 410.98114013671875, 312.5446472167969, 0.8843570351600647, 0.0] 1\n",
            "236 [265.8497619628906, 62.4755859375, 409.5187683105469, 329.08538818359375, 0.9231193661689758, 0.0] 2\n",
            "237 [268.0773620605469, 63.709228515625, 408.0476379394531, 327.9895324707031, 0.913852870464325, 0.0] 2\n",
            "238 [269.47021484375, 61.45159912109375, 401.0867919921875, 327.59930419921875, 0.9090274572372437, 0.0] 2\n",
            "239 [268.661376953125, 58.11865234375, 387.674072265625, 329.4603271484375, 0.9132938385009766, 0.0] 3\n",
            "240 [259.88031005859375, 50.904876708984375, 390.142333984375, 327.9293518066406, 0.9019702672958374, 0.0] 2\n",
            "241 [235.7760009765625, 50.31138610839844, 388.6513671875, 326.74981689453125, 0.8963983058929443, 0.0] 2\n",
            "242 [221.67315673828125, 47.1527099609375, 383.3427734375, 327.5980224609375, 0.9183943867683411, 0.0] 2\n",
            "243 [209.136474609375, 47.94621276855469, 379.6544189453125, 326.84173583984375, 0.8842968940734863, 0.0] 2\n",
            "244 [204.18194580078125, 41.672088623046875, 374.276611328125, 323.5518798828125, 0.8660220503807068, 0.0] 3\n",
            "245 [185.08636474609375, 39.11883544921875, 366.44537353515625, 325.0476989746094, 0.7759849429130554, 0.0] 3\n",
            "246 [210.21365356445312, 42.33381652832031, 369.2956848144531, 323.0804443359375, 0.822716236114502, 0.0] 3\n",
            "247 [195.90609741210938, 37.40837097167969, 361.6422424316406, 325.68609619140625, 0.9106460809707642, 0.0] 3\n",
            "248 [193.61500549316406, 47.808197021484375, 361.718505859375, 322.4612731933594, 0.8890666365623474, 0.0] 2\n",
            "249 [189.3167724609375, 48.25502014160156, 358.894287109375, 323.72882080078125, 0.8995262980461121, 0.0] 3\n",
            "250 [174.53097534179688, 50.903411865234375, 358.5119323730469, 325.99444580078125, 0.8973760008811951, 0.0] 3\n",
            "251 [163.97409057617188, 51.282135009765625, 361.9955749511719, 325.6553039550781, 0.8922247290611267, 0.0] 3\n",
            "252 [198.47962951660156, 52.16606140136719, 370.11822509765625, 362.50555419921875, 0.9208281636238098, 0.0] 3\n",
            "253 [217.5079345703125, 53.05302429199219, 370.69061279296875, 350.81549072265625, 0.8906875848770142, 0.0] 3\n",
            "254 [226.81918334960938, 53.73173522949219, 368.3987121582031, 338.0155029296875, 0.9067184925079346, 0.0] 2\n",
            "255 [232.23077392578125, 53.35028076171875, 363.78936767578125, 351.521484375, 0.9203829765319824, 0.0] 3\n",
            "256 [234.24014282226562, 52.1236572265625, 365.7138366699219, 364.4853210449219, 0.9204061627388, 0.0] 3\n",
            "257 [237.42562866210938, 54.21348571777344, 370.9892883300781, 360.09130859375, 0.9233406782150269, 0.0] 3\n",
            "258 [239.14553833007812, 55.083709716796875, 373.0042419433594, 356.4763488769531, 0.9256779551506042, 0.0] 3\n",
            "259 [239.7282257080078, 54.54310607910156, 370.7918701171875, 360.44293212890625, 0.9293251633644104, 0.0] 3\n",
            "260 [239.39471435546875, 51.346405029296875, 361.51043701171875, 360.5506591796875, 0.927383303642273, 0.0] 3\n",
            "261 [238.4604034423828, 49.95790100097656, 356.56195068359375, 361.64959716796875, 0.9232452511787415, 0.0] 3\n",
            "262 [237.02857971191406, 48.76042175292969, 352.71746826171875, 362.31085205078125, 0.924142599105835, 0.0] 3\n",
            "263 [235.78369140625, 48.03813171386719, 349.62469482421875, 361.84124755859375, 0.934026837348938, 0.0] 3\n",
            "264 [232.2367401123047, 44.59648132324219, 342.00042724609375, 364.941650390625, 0.9238036870956421, 0.0] 3\n",
            "265 [228.91021728515625, 43.48236083984375, 340.11572265625, 365.5314025878906, 0.9278081059455872, 0.0] 3\n",
            "266 [226.9274139404297, 43.52557373046875, 336.4736022949219, 362.0890808105469, 0.9344943165779114, 0.0] 3\n",
            "267 [225.60887145996094, 45.60081481933594, 333.59967041015625, 359.17803955078125, 0.9378447532653809, 0.0] 3\n",
            "268 [222.6000518798828, 41.71986389160156, 332.81512451171875, 364.85723876953125, 0.946774959564209, 0.0] 3\n",
            "269 [221.33779907226562, 41.612457275390625, 332.1508483886719, 366.36065673828125, 0.9428244829177856, 0.0] 3\n",
            "270 [218.48464965820312, 39.40275573730469, 329.0354309082031, 365.4287109375, 0.9255185723304749, 0.0] 3\n",
            "271 [212.09178161621094, 38.048553466796875, 326.52288818359375, 362.52667236328125, 0.9228079915046692, 0.0] 2\n",
            "272 [199.58584594726562, 34.56915283203125, 326.1361999511719, 355.1277160644531, 0.9028058052062988, 0.0] 2\n",
            "273 [197.81631469726562, 32.62071228027344, 326.3528137207031, 350.79583740234375, 0.8669000267982483, 0.0] 2\n",
            "274 [196.1513671875, 33.568267822265625, 325.35467529296875, 346.2311706542969, 0.8751525282859802, 0.0] 2\n",
            "275 [195.78512573242188, 33.570220947265625, 323.7056579589844, 343.0045471191406, 0.8997102975845337, 0.0] 2\n",
            "276 [196.0230712890625, 32.74212646484375, 318.01300048828125, 339.9671325683594, 0.8917133212089539, 0.0] 2\n",
            "277 [194.93138122558594, 31.098495483398438, 315.7553405761719, 342.64764404296875, 0.8928853273391724, 0.0] 2\n",
            "278 [195.830322265625, 29.153640747070312, 313.9791564941406, 342.2271728515625, 0.9082304239273071, 0.0] 2\n",
            "279 [197.72093200683594, 31.14862060546875, 309.45660400390625, 339.94110107421875, 0.901229977607727, 0.0] 2\n",
            "280 [196.14443969726562, 33.229949951171875, 306.1683654785156, 340.1995544433594, 0.8903660178184509, 0.0] 2\n",
            "281 [195.45018005371094, 33.475982666015625, 303.9521484375, 346.68536376953125, 0.8972228765487671, 0.0] 2\n",
            "282 [194.25424194335938, 33.05885314941406, 304.1542663574219, 339.213623046875, 0.9017899036407471, 0.0] 2\n",
            "283 [193.52737426757812, 31.364761352539062, 302.8464050292969, 340.1478271484375, 0.9114254117012024, 0.0] 2\n",
            "284 [194.16986083984375, 29.629104614257812, 307.4580078125, 340.34954833984375, 0.9243620038032532, 0.0] 2\n",
            "285 [195.3238525390625, 28.477996826171875, 308.2012939453125, 341.3175964355469, 0.9258032441139221, 0.0] 2\n",
            "286 [196.17413330078125, 27.921783447265625, 308.83935546875, 340.6968688964844, 0.9264706969261169, 0.0] 2\n",
            "287 [196.70892333984375, 26.82373046875, 309.3232421875, 339.67156982421875, 0.9297819137573242, 0.0] 2\n",
            "288 [197.92327880859375, 25.133697509765625, 311.2451477050781, 339.26873779296875, 0.929180920124054, 0.0] 2\n",
            "289 [198.23751831054688, 24.752578735351562, 312.2629089355469, 339.46954345703125, 0.9307264685630798, 0.0] 2\n",
            "290 [199.1584014892578, 24.136566162109375, 312.2982177734375, 339.9604187011719, 0.929404616355896, 0.0] 2\n",
            "291 [200.01312255859375, 24.191925048828125, 313.08636474609375, 339.07659912109375, 0.9269803166389465, 0.0] 3\n",
            "292 [200.62879943847656, 25.344741821289062, 313.6316833496094, 340.65692138671875, 0.9301722049713135, 0.0] 2\n",
            "293 [200.90380859375, 25.78271484375, 314.0526123046875, 341.3371276855469, 0.9296814799308777, 0.0] 2\n",
            "294 [201.39111328125, 25.757186889648438, 314.97918701171875, 341.08721923828125, 0.929114043712616, 0.0] 2\n",
            "295 [201.4539794921875, 25.617645263671875, 315.1844482421875, 340.5495300292969, 0.9288553595542908, 0.0] 2\n",
            "296 [201.63116455078125, 25.166793823242188, 315.1390380859375, 340.81005859375, 0.9310182332992554, 0.0] 2\n",
            "297 [201.6172637939453, 25.26171875, 314.9039306640625, 341.7272033691406, 0.930763304233551, 0.0] 2\n",
            "298 [201.6408233642578, 25.283782958984375, 314.71527099609375, 341.4912414550781, 0.9318970441818237, 0.0] 2\n",
            "299 [201.43849182128906, 25.310302734375, 314.05572509765625, 341.8185119628906, 0.9309918284416199, 0.0] 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}